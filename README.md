# DistilBERT Fine-Tuning for Narratvie Classification

This project fine-tunes a DistilBERT language model on a custom dataset for [brief task description – e.g., binary text classification of medical narratives]. 
It’s open-source and built for collaborative improvement and experimentation.

## Goals

- Fine-tune DistilBERT efficiently on a real-world classification task
- Support experimentation with different datasets, hyperparameters, and adapters (like LoRA or PEFT)
- Encourage contributions and reproducibility in the open-source NLP space

## Getting Started

1. Clone the repo:


2. (Recommended) Create a virtual environment:


3. Install dependencies:


4. Run the training script:


